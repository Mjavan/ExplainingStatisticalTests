{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data and Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.20 (default, Oct  3 2024, 15:24:27) \n",
      "[GCC 11.2.0]\n",
      "Python executable: /sc/home/masoumeh.javanbakhat/conda3/envs/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path that data has been saved is as follows:\n",
    "# /sc/dhc-cold/groups/fglippert => but the permission to dhc-cold has been denied \n",
    "IMG_DIR = \"/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256\"\n",
    "IMG_DIR = os.path.join(IMG_DIR, \"group_by_hippocampus\")\n",
    "ANNOTATION_FILE = \"adni_T1_3T_linear_annotation.csv\"\n",
    "ANNOTATION_PATH = os.path.join(IMG_DIR, ANNOTATION_FILE)\n",
    "COLS = [\"archive_fname\", \"group\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdniMRIDataset2D(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[\"archive_fname\"].iloc[idx])\n",
    "        #print(img_path)\n",
    "        image = self.read_image(img_path)\n",
    "        label = self.img_labels[\"group\"].iloc[idx]\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "    def read_image(self,path):\n",
    "        img = nib.load(path).get_fdata().astype(np.uint8)\n",
    "        if img.ndim == 2:\n",
    "            img = img[:, :, np.newaxis] ## HW -> HWC\n",
    "        img = img.transpose(2, 0, 1) ## HWC -> CHW\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_dataset = AdniMRIDataset2D(annotations_file=ANNOTATION_PATH, img_dir=IMG_DIR)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(dataset): \n",
    "    group0 = []\n",
    "    group1 = []\n",
    "    \n",
    "    for img,label in dataset:\n",
    "        if label==0:\n",
    "            group0.append(img)    \n",
    "        else:\n",
    "            group1.append(img)\n",
    "            \n",
    "    group0 = np.concatenate(group0, axis=0)        \n",
    "    group1 = np.concatenate(group1, axis=0)\n",
    "    \n",
    "    return(group0, group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0, group1 = get_groups(adni_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trianed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the pretariend model along with weighst\n",
    "\n",
    "- Extracting embeddings from the last layer on groups \n",
    "\n",
    "- Checking statitical test to see if they are statistically different\n",
    "\n",
    "- Uisng Grad-Cam for back-probabgating test-statistic and visualise differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "HID_DIM = 2048\n",
    "OUT_DIM = 128\n",
    "\n",
    "\n",
    "#### Renet backbone\n",
    "class resnet50_fext(nn.Module):\n",
    "    def __init__(self,pretarin=True):  \n",
    "        super(resnet50,self).__init__()\n",
    "        if pretarin:\n",
    "            backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0],-1)\n",
    "        return(embedding)\n",
    "    \n",
    "    \n",
    "#### Linear model\n",
    "class MLP(nn.Module): \n",
    "    def __init__(self,in_dim,mlp_hid_size,proj_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.head = nn.Sequential(nn.Linear(in_dim,mlp_hid_size),\n",
    "                                 nn.BatchNorm1d(mlp_hid_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(mlp_hid_size,proj_size))        \n",
    "    def forward(self,x):\n",
    "        x= self.head(x)\n",
    "        return(x)\n",
    "    \n",
    "#### Byol model\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self,net,backbone,hid_dim,out_dim):  \n",
    "        super(BYOL,self).__init__()\n",
    "        self.net = net\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.projection = MLP(in_dim= backbone.fc.in_features,mlp_hid_size=hid_dim,proj_size=out_dim) \n",
    "        self.prediction = MLP(in_dim= out_dim,mlp_hid_size=hid_dim,proj_size=out_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0],-1)\n",
    "        project = self.projection(embedding)\n",
    "        \n",
    "        if self.net=='target':\n",
    "            return(project)\n",
    "        predict = self.prediction(project)\n",
    "        return(predict)\n",
    "    \n",
    "#### SimCLR model    \n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self,backbone,hid_dim,out_dim):  \n",
    "        super(SimCLR,self).__init__()\n",
    "        # we get representations from avg_pooling layer\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.projection = MLP(in_dim= backbone.fc.in_features,mlp_hid_size=hid_dim,proj_size=out_dim) \n",
    "\n",
    "    def forward(self,x):        \n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0],-1)\n",
    "        project = self.projection(embedding)\n",
    "        return(project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= \"/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes\"\n",
    "\n",
    "root_dir = Path(base_path) \n",
    "\n",
    "checkpoint_dir = root_dir / 'self_supervised' / 'simclr' / 'simclr_ckpts'\n",
    "\n",
    "pre_exp= 2\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_dir_last = os.path.join(checkpoint_dir,f'{pre_exp}_last_sclr.pt')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(sam_dir_last,map_location=device)\n",
    "\n",
    "## Load model\n",
    "\n",
    "backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "model = SimCLR(backbone,hid_dim=2048,out_dim=128).to(device)\n",
    "\n",
    "model.load_state_dict(state_dict['model'])\n",
    "\n",
    "encoder = torch.nn.Sequential(*list(model.children())[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I can add a FC layer if I want to reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to use a smaller dimension I can use a FC layer as follows \n",
    "\n",
    "\n",
    "reduced_dim = 512  # You can adjust this as needed, e.g., 512, 256, 128\n",
    "fc_layer = nn.Linear(2048, reduced_dim) \n",
    "\n",
    "# it outputs hidden representations of dimension 512\n",
    "reduced_encoder = nn.Sequential(encoder, fc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "encoder_output = encoder(sample_input)\n",
    "encoder_output = encoder_output.squeeze()  # This removes the extra dimension, if any\n",
    "latent_representation = fc_layer(encoder_output)\n",
    "print(latent_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To get embeddings I can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_numpy(encoder, images_np, device,gr=1,batch_size=32):\n",
    "    \"\"\"Takes a numpy array of images, extract embedding vectors, return embeddings\"\"\"\n",
    "    encoder.eval()\n",
    "    embeddings_list = []\n",
    "    \n",
    "    # ImageNet mean and std values\n",
    "    IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)  \n",
    "    IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)  \n",
    "    \n",
    "    # Convert numpy array to torch tensor and move to device\n",
    "    images_tensor = torch.tensor(images_np).unsqueeze(1).to(device).float()  # Assuming images_np is of shape (n_samples, 1, 256, 256)\n",
    "    images_tensor = images_tensor.repeat(1, 3, 1, 1) # create image with 3 channels\n",
    "    \n",
    "    # Apply ImageNet normalization\n",
    "    images_tensor = (images_tensor - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    \n",
    "    reduced_dim = 512  # You can adjust this as needed, e.g., 512, 256, 128\n",
    "    fc_layer = nn.Linear(2048, reduced_dim)\n",
    "    \n",
    "    print(f'shape of tensor:{images_tensor.shape}')\n",
    "    # If you want to process the data in batches\n",
    "    num_samples = images_tensor.size(0)\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = images_tensor[i:i + batch_size]\n",
    "        \n",
    "        # Ensure the batch is on the right device\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Get embeddings from the model\n",
    "        with torch.no_grad():\n",
    "            embeddings = encoder(batch)\n",
    "            embeddings = embeddings.view(embeddings.size(0), -1)  # Flatten embeddings if needed\n",
    "            #embeddings = fc_layer(embeddings)\n",
    "        # Append the embeddings\n",
    "        embeddings_list.append(embeddings.cpu().numpy())\n",
    "        \n",
    "    n = embeddings_list[0].shape[1]\n",
    "    \n",
    "    path_embed = os.path.join('./adni_embed',f'embed_gr{gr}_{n}_nr.npy')\n",
    "    \n",
    "    gr_embed = np.vstack(embeddings_list)\n",
    "    \n",
    "    print(f'shape of embeddings:{gr_embed.shape}')\n",
    "    \n",
    "    np.save(path_embed,gr_embed)\n",
    "\n",
    "    return gr_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0_embed_nr = get_embeddings_from_numpy(encoder=encoder, images_np=group0, device=device,gr=1,batch_size=64)\n",
    "\n",
    "group1_embed_nr = get_embeddings_from_numpy(encoder=encoder, images_np=group1, device=device,gr=2,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now I can define path and save embeddings as follows:\n",
    "\n",
    "- Note that you should change path as I used this path befoe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gr1_embed = os.path.join(path,'embed_gr1.npy')\n",
    "\n",
    "path_gr2_embed = os.path.join(path,'embed_gr2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, let's save embeddings of each group\n",
    "\n",
    "np.save(path_gr1_embed,group0_embed)\n",
    "\n",
    "np.save(path_gr2_embed,group1_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDTest:\n",
    "    def __init__(self, features_X, features_Y, n_perm=1000):\n",
    "        \n",
    "        self.n_perm = n_perm\n",
    "        self.features_X = features_X\n",
    "        self.features_Y = features_Y\n",
    "        \n",
    "    def _compute_mmd(self, features_X, features_Y):\n",
    "        \n",
    "        mean_fX = features_X.mean(0)\n",
    "        mean_fY = features_Y.mean(0)\n",
    "        D = mean_fX - mean_fY\n",
    "        statistic = np.linalg.norm(D)**2\n",
    "        return statistic\n",
    "    \n",
    "    def _compute_p_value(self):\n",
    "        \n",
    "        # compute real test statistic\n",
    "        stat = self._compute_mmd(self.features_X, self.features_Y)\n",
    "        n, m = len(self.features_X), len(self.features_Y)\n",
    "        l = n + m\n",
    "        features_Z = np.vstack((self.features_X, self.features_Y))\n",
    "        \n",
    "        # compute null samples\n",
    "        resampled_vals = np.empty(self.n_perm)\n",
    "        for i in range(self.n_perm):\n",
    "            index = np.random.permutation(l) # it permutes indices from 0 to l\n",
    "            feats_X, feats_Y = features_Z[index[:n]], features_Z[index[n:]]\n",
    "            resampled_vals[i] = self._compute_mmd(feats_X, feats_Y)\n",
    "            \n",
    "        resampled_vals.sort()\n",
    "        #p_val = np.mean(stat < resampled_vals)\n",
    "        p_val = (np.sum(stat<= resampled_vals)+1)/(self.n_perm+1)\n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd = MMDTest(group0_embed, group1_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1_embed = np.load(path_gr1_embed)\n",
    "\n",
    "gr2_embed = np.load(path_gr2_embed)\n",
    "\n",
    "print(gr1_embed.shape)\n",
    "\n",
    "print(gr2_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting mean embeddings and backprobagating test-statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "import gradcam\n",
    "\n",
    "from gradcam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my first code (old code)\n",
    "\n",
    "def get_mean_embeddings(gcam, dataloader, device, latent_dim=2048):\n",
    "        \"\"\"Takes dataloader of each group, extract embedding vectors, return mean embeddings\"\"\"\n",
    "        # Initialize accumulators for healthy and unhealthy groups\n",
    "        sum_f = torch.zeros_like(torch.zeros(latent_dim)).to(device)  \n",
    "        count_f = 0 # Count of samples in each group\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = gcam.forward(images)\n",
    "            embeddings = embeddings.view(embeddings.size()[0],-1)\n",
    "            sum_f += embeddings.sum(dim=0)  # Sum of embeddings for this batch\n",
    "            count_f += embeddings.size(0) \n",
    "        mean_embed = sum_f / count_f if count_f > 0 else torch.zeros_like(sum_f) \n",
    "        del sum_f, embeddings, images\n",
    "        torch.cuda.empty_cache()\n",
    "        return mean_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(group, device):\n",
    "    group_tensor = torch.tensor(group).unsqueeze(1).to(device).float()  # Assuming images_np is of shape (n_samples, 1, 256, 256)\n",
    "    group_tensor = group_tensor.repeat(1, 3, 1, 1) # create image with 3 channels\n",
    "    \n",
    "    IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)  \n",
    "    IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    # Apply ImageNet normalization\n",
    "    group_tensor = (group_tensor - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    \n",
    "    return(group_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(group0, group1,device, bs=64):\n",
    "    group0_tensor = convert_to_tensor(group0, device)\n",
    "    group1_tensor = convert_to_tensor(group1, device)\n",
    "    \n",
    "    print(group0_tensor.shape)\n",
    "    print(group1_tensor.shape)\n",
    "    \n",
    "    group0_loader = DataLoader(group0_tensor, batch_size=bs, shuffle=False, drop_last=True)\n",
    "    group1_loader = DataLoader(group1_tensor, batch_size=bs, shuffle=False, drop_last=True)\n",
    "    \n",
    "    return(group0_loader, group1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr0_nr_loader, gr1_nr_loader = get_loader(gr0_100, gr1_100,device, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprobagating test statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprobagate_statistics(model,group0, group1,target_layer, bs, device):\n",
    "    \n",
    "    \"\"\"Calculate the test statistic for different groups of DR.\"\"\"\n",
    "    # convert numpy groups to tensors\n",
    "    group0_loader, group1_loader = get_loader(group0, group1,device, bs=64)\n",
    "    \n",
    "    gcam = GradCAM(model, target_layer=target_layer, relu=True, device=device)\n",
    "    # Calculate mean embeddings\n",
    "    group0_mean = get_mean_embeddings(gcam,group0_loader,device)\n",
    "    group1_mean = get_mean_embeddings(gcam,group1_loader,device)\n",
    "    D = group0_mean - group1_mean\n",
    "    print(f'group0_mean:{group0_mean.shape}')\n",
    "    print(f'group1_mean:{group1_mean.shape}')\n",
    "    test_statistic = torch.norm(D, p=2)\n",
    "    return(test_statistic, D, gcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attributions(dataloader,gcam,device,backprop_value):\n",
    "    \n",
    "    \"\"\"Process and return GradCAM attributions in batches.\"\"\"\n",
    "    attributions_list = []\n",
    "    embed_list = [] # save embeddings as numpy array\n",
    "    # Compute attribution maps for each group\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = gcam.forward(images)\n",
    "        embeddings = embeddings.view(embeddings.size()[0],-1).cpu().data.numpy()\n",
    "        embed_list.append(embeddings)\n",
    "        del embeddings\n",
    "        gcam.backward(backprop_value)\n",
    "        attributions = gcam.generate()\n",
    "        attributions = attributions.squeeze().cpu().data.numpy()\n",
    "        attributions_list.append(attributions)\n",
    "    return np.vstack(attributions_list), np.vstack(embed_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model,group0,group1,device,target_layer,bs=64,backprop_type='test_statistic',latent_dim_idx=None):\n",
    "    \n",
    "    \"\"\"Main experiment function.\"\"\"\n",
    "    test_statistic, D, gcam = backprobagate_statistics(model, group0, group1,target_layer, bs, device)\n",
    "\n",
    "    backprop_value = test_statistic\n",
    "    \n",
    "    group0_loader, group1_loader = get_loader(group0, group1,device)\n",
    "        \n",
    "    group0_attr, group0_embed = process_attributions(group0_loader, gcam, backprop_value)\n",
    "    group1_attr, group1_embed = process_attributions(group1_loader, gcam, backprop_value) \n",
    "        \n",
    "    #save_attributions(group0_attr, group1_attr,latent_dim_idx)\n",
    "    \n",
    "    print(f'gr1:{group0_attr.shape}')\n",
    "    print(f'gr2:{group2_attr.shape}')\n",
    "    \n",
    "    return(group0_attr, group1_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imorting heatmaps and looking at them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroGradientError(Exception):\n",
    "    \"\"\"Custom exception to handle cases where the gradient is zero.\"\"\"\n",
    "    pass\n",
    "\n",
    "def save_cam_with_alpha(image, gcam, alpha=0.5):\n",
    "    \n",
    "    # Convert grayscale image to 3 channels if needed\n",
    "    if len(image.shape) == 2:  # Grayscale image (H, W)\n",
    "        image = np.stack([image] * 3, axis=-1)  # Convert to (H, W, 3)\n",
    "    \n",
    "    # Normalize the Grad-CAM values to [0, 1]\n",
    "    # Normalize the Grad-CAM values to [0, 1], handling zero gradients\n",
    "    gcam_min = np.min(gcam)\n",
    "    gcam_max = np.max(gcam)\n",
    "    \n",
    "    try:\n",
    "        if gcam_max == gcam_min:  # If all values are zero, raise an error\n",
    "            raise ZeroGradientError(\"Gradient map contains only zero values, cannot overlay.\")\n",
    "        gcam = (gcam - gcam_min) / (gcam_max - gcam_min)  # Normalize gradient map\n",
    "    except ZeroGradientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # Handle the error (for example, return the original image or skip processing)\n",
    "        return image, image  # Return the original image if error occurs\n",
    "    \n",
    "    # Resize Grad-CAM to match the image dimensions (224x224)\n",
    "    h, w = image.shape[:2]  # Get height and width (height, width) from image shape\n",
    "    gcam_resized = np.array(Image.fromarray(gcam).resize((w, h), Image.BILINEAR))\n",
    "\n",
    "    # Apply a colormap (similar to cv2.applyColorMap)\n",
    "    gcam_colored = plt.cm.jet(gcam_resized)[:, :, :3] * 255  # Apply colormap and select RGB channels\n",
    "    gcam_colored = gcam_colored.astype(np.uint8)\n",
    "\n",
    "    # Add Grad-CAM on top of the original image using alpha blending\n",
    "    heatmap = gcam_colored.astype(np.float64)\n",
    "    \n",
    "    # checking dimension of image and heatmaps\n",
    "    print(f'heatmap:{heatmap.shape}')\n",
    "    print(f'image:{image.shape}')\n",
    "    \n",
    "    overlaid_image = (alpha * heatmap + (1 - alpha) * image.astype(np.float64)).astype(np.uint8)\n",
    "\n",
    "    return image, overlaid_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the 'src' directory\n",
    "src_path = os.path.abspath(\"src\")\n",
    "\n",
    "# Add 'src' to system path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
