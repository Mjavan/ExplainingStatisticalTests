{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data and Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.20 (default, Oct  3 2024, 15:24:27) \n",
      "[GCC 11.2.0]\n",
      "Python executable: /sc/home/masoumeh.javanbakhat/conda3/envs/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The path that data has been saved is as follows:\n",
    "# /sc/dhc-cold/groups/fglippert => but the permission to dhc-cold has been denied\n",
    "IMG_DIR_OLD = \"/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256\"\n",
    "IMG_DIR = '/sc/projects/sci-lippert/chair/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256'\n",
    "IMG_DIR = os.path.join(IMG_DIR, \"group_by_hippocampus\")\n",
    "ANNOTATION_FILE = \"adni_T1_3T_linear_annotation.csv\"\n",
    "ANNOTATION_PATH = os.path.join(IMG_DIR, ANNOTATION_FILE)\n",
    "COLS = [\"archive_fname\", \"group\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdniMRIDataset2D(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(\n",
    "            self.img_dir, self.img_labels[\"archive_fname\"].iloc[idx])\n",
    "        # print(img_path)\n",
    "        image = self.read_image(img_path)\n",
    "        label = self.img_labels[\"group\"].iloc[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = nib.load(path).get_fdata().astype(np.uint8)\n",
    "        if img.ndim == 2:\n",
    "            img = img[:, :, np.newaxis]  # HW -> HWC\n",
    "        img = img.transpose(2, 0, 1)  # HWC -> CHW\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_dataset = AdniMRIDataset2D(\n",
    "    annotations_file=ANNOTATION_PATH, img_dir=IMG_DIR)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path not available!\n",
    "As it's apparent from the previous cell, the location \"/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256/group_by_hippocampus/adni_T1_3T_linear_annotation.csv\" doesn't exist!\n",
    "Then we should find the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(dataset):\n",
    "    group0 = []\n",
    "    group1 = []\n",
    "\n",
    "    for img, label in dataset:\n",
    "        if label == 0:\n",
    "            group0.append(img)\n",
    "        else:\n",
    "            group1.append(img)\n",
    "\n",
    "    group0 = np.concatenate(group0, axis=0)\n",
    "    group1 = np.concatenate(group1, axis=0)\n",
    "\n",
    "    return (group0, group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0, group1 = get_groups(adni_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4591, 256, 256)\n",
      "(4592, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(group0.shape)\n",
    "print(group1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trianed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the pretariend model along with weighst\n",
    "\n",
    "- Extracting embeddings from the last layer on groups \n",
    "\n",
    "- Checking statitical test to see if they are statistically different\n",
    "\n",
    "- Uisng Grad-Cam for back-probabgating test-statistic and visualise differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "HID_DIM = 2048\n",
    "OUT_DIM = 128\n",
    "\n",
    "\n",
    "# Renet backbone\n",
    "class resnet50_fext(nn.Module):\n",
    "    def __init__(self, pretarin=True):\n",
    "        super(resnet50, self).__init__()\n",
    "        if pretarin:\n",
    "            backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0], -1)\n",
    "        return (embedding)\n",
    "\n",
    "\n",
    "# Linear model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, mlp_hid_size, proj_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.head = nn.Sequential(nn.Linear(in_dim, mlp_hid_size),\n",
    "                                  nn.BatchNorm1d(mlp_hid_size),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(mlp_hid_size, proj_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        return (x)\n",
    "\n",
    "# Byol model\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self, net, backbone, hid_dim, out_dim):\n",
    "        super(BYOL, self).__init__()\n",
    "        self.net = net\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.projection = MLP(in_dim=backbone.fc.in_features,\n",
    "                              mlp_hid_size=hid_dim, proj_size=out_dim)\n",
    "        self.prediction = MLP(\n",
    "            in_dim=out_dim, mlp_hid_size=hid_dim, proj_size=out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0], -1)\n",
    "        project = self.projection(embedding)\n",
    "\n",
    "        if self.net == 'target':\n",
    "            return (project)\n",
    "        predict = self.prediction(project)\n",
    "        return (predict)\n",
    "\n",
    "# SimCLR model\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, backbone, hid_dim, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        # we get representations from avg_pooling layer\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.projection = MLP(in_dim=backbone.fc.in_features,\n",
    "                              mlp_hid_size=hid_dim, proj_size=out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0], -1)\n",
    "        project = self.projection(embedding)\n",
    "        return (project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3802382/30374742.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(sam_dir_last, map_location=device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes/self_supervised/simclr/simclr_ckpts/2_last_sclr.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m sam_dir_last \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_exp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_last_sclr.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msam_dir_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m backbone \u001b[38;5;241m=\u001b[39m resnet50(weights\u001b[38;5;241m=\u001b[39mResNet50_Weights\u001b[38;5;241m.\u001b[39mIMAGENET1K_V2)\n",
      "File \u001b[0;32m~/conda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/conda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/conda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes/self_supervised/simclr/simclr_ckpts/2_last_sclr.pt'"
     ]
    }
   ],
   "source": [
    "base_path = \"/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes\"\n",
    "\n",
    "root_dir = Path(base_path)\n",
    "\n",
    "checkpoint_dir = root_dir / 'self_supervised' / 'simclr' / 'simclr_ckpts'\n",
    "\n",
    "pre_exp = 2\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_dir_last = os.path.join(checkpoint_dir, f'{pre_exp}_last_sclr.pt')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(sam_dir_last, map_location=device)\n",
    "\n",
    "# Load model\n",
    "\n",
    "backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "model = SimCLR(backbone, hid_dim=2048, out_dim=128).to(device)\n",
    "\n",
    "model.load_state_dict(state_dict['model'])\n",
    "\n",
    "encoder = torch.nn.Sequential(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I can add a FC layer if I want to reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to use a smaller dimension I can use a FC layer as follows\n",
    "\n",
    "\n",
    "reduced_dim = 512  # You can adjust this as needed, e.g., 512, 256, 128\n",
    "fc_layer = nn.Linear(2048, reduced_dim)\n",
    "\n",
    "# it outputs hidden representations of dimension 512\n",
    "reduced_encoder = nn.Sequential(encoder, fc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "encoder_output = encoder(sample_input)\n",
    "# This removes the extra dimension, if any\n",
    "encoder_output = encoder_output.squeeze()\n",
    "latent_representation = fc_layer(encoder_output)\n",
    "print(latent_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To get embeddings I can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_numpy(encoder, images_np, device, gr=1, batch_size=32):\n",
    "    \"\"\"Takes a numpy array of images, extract embedding vectors, return embeddings\"\"\"\n",
    "    encoder.eval()\n",
    "    embeddings_list = []\n",
    "\n",
    "    # ImageNet mean and std values\n",
    "    IMAGENET_MEAN = torch.tensor(\n",
    "        [0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    IMAGENET_STD = torch.tensor(\n",
    "        [0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    # Convert numpy array to torch tensor and move to device\n",
    "    # Assuming images_np is of shape (n_samples, 1, 256, 256)\n",
    "    images_tensor = torch.tensor(images_np).unsqueeze(1).to(device).float()\n",
    "    images_tensor = images_tensor.repeat(\n",
    "        1, 3, 1, 1)  # create image with 3 channels\n",
    "\n",
    "    # Apply ImageNet normalization\n",
    "    images_tensor = (images_tensor - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "    reduced_dim = 512  # You can adjust this as needed, e.g., 512, 256, 128\n",
    "    fc_layer = nn.Linear(2048, reduced_dim)\n",
    "\n",
    "    print(f'shape of tensor:{images_tensor.shape}')\n",
    "    # If you want to process the data in batches\n",
    "    num_samples = images_tensor.size(0)\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = images_tensor[i:i + batch_size]\n",
    "\n",
    "        # Ensure the batch is on the right device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get embeddings from the model\n",
    "        with torch.no_grad():\n",
    "            embeddings = encoder(batch)\n",
    "            # Flatten embeddings if needed\n",
    "            embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "            # embeddings = fc_layer(embeddings)\n",
    "        # Append the embeddings\n",
    "        embeddings_list.append(embeddings.cpu().numpy())\n",
    "\n",
    "    n = embeddings_list[0].shape[1]\n",
    "\n",
    "    path_embed = os.path.join('./adni_embed', f'embed_gr{gr}_{n}_nr.npy')\n",
    "\n",
    "    gr_embed = np.vstack(embeddings_list)\n",
    "\n",
    "    print(f'shape of embeddings:{gr_embed.shape}')\n",
    "\n",
    "    np.save(path_embed, gr_embed)\n",
    "\n",
    "    return gr_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0_embed_nr = get_embeddings_from_numpy(\n",
    "    encoder=encoder, images_np=group0, device=device, gr=1, batch_size=64)\n",
    "\n",
    "group1_embed_nr = get_embeddings_from_numpy(\n",
    "    encoder=encoder, images_np=group1, device=device, gr=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now I can define path and save embeddings as follows:\n",
    "\n",
    "- Note that you should change path as I used this path befoe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gr1_embed = os.path.join(path, 'embed_gr1.npy')\n",
    "\n",
    "path_gr2_embed = os.path.join(path, 'embed_gr2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's save embeddings of each group\n",
    "\n",
    "np.save(path_gr1_embed, group0_embed)\n",
    "\n",
    "np.save(path_gr2_embed, group1_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDTest:\n",
    "    def __init__(self, features_X, features_Y, n_perm=1000):\n",
    "\n",
    "        self.n_perm = n_perm\n",
    "        self.features_X = features_X\n",
    "        self.features_Y = features_Y\n",
    "\n",
    "    def _compute_mmd(self, features_X, features_Y):\n",
    "\n",
    "        mean_fX = features_X.mean(0)\n",
    "        mean_fY = features_Y.mean(0)\n",
    "        D = mean_fX - mean_fY\n",
    "        statistic = np.linalg.norm(D)**2\n",
    "        return statistic\n",
    "\n",
    "    def _compute_p_value(self):\n",
    "\n",
    "        # compute real test statistic\n",
    "        stat = self._compute_mmd(self.features_X, self.features_Y)\n",
    "        n, m = len(self.features_X), len(self.features_Y)\n",
    "        l = n + m\n",
    "        features_Z = np.vstack((self.features_X, self.features_Y))\n",
    "\n",
    "        # compute null samples\n",
    "        resampled_vals = np.empty(self.n_perm)\n",
    "        for i in range(self.n_perm):\n",
    "            index = np.random.permutation(l)  # it permutes indices from 0 to l\n",
    "            feats_X, feats_Y = features_Z[index[:n]], features_Z[index[n:]]\n",
    "            resampled_vals[i] = self._compute_mmd(feats_X, feats_Y)\n",
    "\n",
    "        resampled_vals.sort()\n",
    "        # p_val = np.mean(stat < resampled_vals)\n",
    "        p_val = (np.sum(stat <= resampled_vals)+1)/(self.n_perm+1)\n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd = MMDTest(group0_embed, group1_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1_embed = np.load(path_gr1_embed)\n",
    "\n",
    "gr2_embed = np.load(path_gr2_embed)\n",
    "\n",
    "print(gr1_embed.shape)\n",
    "\n",
    "print(gr2_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting mean embeddings and backprobagating test-statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradcam import GradCAM\n",
    "import gradcam\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my first code (old code)\n",
    "\n",
    "def get_mean_embeddings(gcam, dataloader, device, latent_dim=2048):\n",
    "    \"\"\"Takes dataloader of each group, extract embedding vectors, return mean embeddings\"\"\"\n",
    "    # Initialize accumulators for healthy and unhealthy groups\n",
    "    sum_f = torch.zeros_like(torch.zeros(latent_dim)).to(device)\n",
    "    count_f = 0  # Count of samples in each group\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = gcam.forward(images)\n",
    "        embeddings = embeddings.view(embeddings.size()[0], -1)\n",
    "        sum_f += embeddings.sum(dim=0)  # Sum of embeddings for this batch\n",
    "        count_f += embeddings.size(0)\n",
    "    mean_embed = sum_f / count_f if count_f > 0 else torch.zeros_like(sum_f)\n",
    "    del sum_f, embeddings, images\n",
    "    torch.cuda.empty_cache()\n",
    "    return mean_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(group, device):\n",
    "    # Assuming images_np is of shape (n_samples, 1, 256, 256)\n",
    "    group_tensor = torch.tensor(group).unsqueeze(1).to(device).float()\n",
    "    group_tensor = group_tensor.repeat(\n",
    "        1, 3, 1, 1)  # create image with 3 channels\n",
    "\n",
    "    IMAGENET_MEAN = torch.tensor(\n",
    "        [0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    IMAGENET_STD = torch.tensor(\n",
    "        [0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    # Apply ImageNet normalization\n",
    "    group_tensor = (group_tensor - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "    return (group_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(group0, group1, device, bs=64):\n",
    "    group0_tensor = convert_to_tensor(group0, device)\n",
    "    group1_tensor = convert_to_tensor(group1, device)\n",
    "\n",
    "    print(group0_tensor.shape)\n",
    "    print(group1_tensor.shape)\n",
    "\n",
    "    group0_loader = DataLoader(\n",
    "        group0_tensor, batch_size=bs, shuffle=False, drop_last=True)\n",
    "    group1_loader = DataLoader(\n",
    "        group1_tensor, batch_size=bs, shuffle=False, drop_last=True)\n",
    "\n",
    "    return (group0_loader, group1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr0_nr_loader, gr1_nr_loader = get_loader(gr0_100, gr1_100, device, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprobagating test statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprobagate_statistics(model, group0, group1, target_layer, bs, device):\n",
    "    \"\"\"Calculate the test statistic for different groups of DR.\"\"\"\n",
    "    # convert numpy groups to tensors\n",
    "    group0_loader, group1_loader = get_loader(group0, group1, device, bs=64)\n",
    "\n",
    "    gcam = GradCAM(model, target_layer=target_layer, relu=True, device=device)\n",
    "    # Calculate mean embeddings\n",
    "    group0_mean = get_mean_embeddings(gcam, group0_loader, device)\n",
    "    group1_mean = get_mean_embeddings(gcam, group1_loader, device)\n",
    "    D = group0_mean - group1_mean\n",
    "    print(f'group0_mean:{group0_mean.shape}')\n",
    "    print(f'group1_mean:{group1_mean.shape}')\n",
    "    test_statistic = torch.norm(D, p=2)\n",
    "    return (test_statistic, D, gcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attributions(dataloader, gcam, device, backprop_value):\n",
    "    \"\"\"Process and return GradCAM attributions in batches.\"\"\"\n",
    "    attributions_list = []\n",
    "    embed_list = []  # save embeddings as numpy array\n",
    "    # Compute attribution maps for each group\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = gcam.forward(images)\n",
    "        embeddings = embeddings.view(\n",
    "            embeddings.size()[0], -1).cpu().data.numpy()\n",
    "        embed_list.append(embeddings)\n",
    "        del embeddings\n",
    "        gcam.backward(backprop_value)\n",
    "        attributions = gcam.generate()\n",
    "        attributions = attributions.squeeze().cpu().data.numpy()\n",
    "        attributions_list.append(attributions)\n",
    "    return np.vstack(attributions_list), np.vstack(embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, group0, group1, device, target_layer, bs=64, backprop_type='test_statistic', latent_dim_idx=None):\n",
    "    \"\"\"Main experiment function.\"\"\"\n",
    "    test_statistic, D, gcam = backprobagate_statistics(\n",
    "        model, group0, group1, target_layer, bs, device)\n",
    "\n",
    "    backprop_value = test_statistic\n",
    "\n",
    "    group0_loader, group1_loader = get_loader(group0, group1, device)\n",
    "\n",
    "    group0_attr, group0_embed = process_attributions(\n",
    "        group0_loader, gcam, backprop_value)\n",
    "    group1_attr, group1_embed = process_attributions(\n",
    "        group1_loader, gcam, backprop_value)\n",
    "\n",
    "    # save_attributions(group0_attr, group1_attr,latent_dim_idx)\n",
    "\n",
    "    print(f'gr1:{group0_attr.shape}')\n",
    "    print(f'gr2:{group2_attr.shape}')\n",
    "\n",
    "    return (group0_attr, group1_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imorting heatmaps and looking at them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroGradientError(Exception):\n",
    "    \"\"\"Custom exception to handle cases where the gradient is zero.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def save_cam_with_alpha(image, gcam, alpha=0.5):\n",
    "\n",
    "    # Convert grayscale image to 3 channels if needed\n",
    "    if len(image.shape) == 2:  # Grayscale image (H, W)\n",
    "        image = np.stack([image] * 3, axis=-1)  # Convert to (H, W, 3)\n",
    "\n",
    "    # Normalize the Grad-CAM values to [0, 1]\n",
    "    # Normalize the Grad-CAM values to [0, 1], handling zero gradients\n",
    "    gcam_min = np.min(gcam)\n",
    "    gcam_max = np.max(gcam)\n",
    "\n",
    "    try:\n",
    "        if gcam_max == gcam_min:  # If all values are zero, raise an error\n",
    "            raise ZeroGradientError(\n",
    "                \"Gradient map contains only zero values, cannot overlay.\")\n",
    "        # Normalize gradient map\n",
    "        gcam = (gcam - gcam_min) / (gcam_max - gcam_min)\n",
    "    except ZeroGradientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # Handle the error (for example, return the original image or skip processing)\n",
    "        return image, image  # Return the original image if error occurs\n",
    "\n",
    "    # Resize Grad-CAM to match the image dimensions (224x224)\n",
    "    # Get height and width (height, width) from image shape\n",
    "    h, w = image.shape[:2]\n",
    "    gcam_resized = np.array(Image.fromarray(\n",
    "        gcam).resize((w, h), Image.BILINEAR))\n",
    "\n",
    "    # Apply a colormap (similar to cv2.applyColorMap)\n",
    "    # Apply colormap and select RGB channels\n",
    "    gcam_colored = plt.cm.jet(gcam_resized)[:, :, :3] * 255\n",
    "    gcam_colored = gcam_colored.astype(np.uint8)\n",
    "\n",
    "    # Add Grad-CAM on top of the original image using alpha blending\n",
    "    heatmap = gcam_colored.astype(np.float64)\n",
    "\n",
    "    # checking dimension of image and heatmaps\n",
    "    print(f'heatmap:{heatmap.shape}')\n",
    "    print(f'image:{image.shape}')\n",
    "\n",
    "    overlaid_image = (alpha * heatmap + (1 - alpha) *\n",
    "                      image.astype(np.float64)).astype(np.uint8)\n",
    "\n",
    "    return image, overlaid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the 'src' directory\n",
    "src_path = os.path.abspath(\"src\")\n",
    "\n",
    "# Add 'src' to system path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New way to solve OOM problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://chatgpt.com/c/683eac2a-0d4c-8005-a5b7-71fd71ce4a9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_group_embeddings_and_grads(gcam, dataloader, device):\n",
    "    \"\"\"\n",
    "    For a group, compute:\n",
    "    - accumulated sum of embeddings (detached)\n",
    "    - total number of samples\n",
    "    - accumulated gradients w.r.t. input images (detached)\n",
    "    \"\"\"\n",
    "    latent_dim = 2048  # or whatever your embedding size is\n",
    "    group_sum = torch.zeros(latent_dim, device=device)\n",
    "    group_count = 0\n",
    "    grads_sum = None\n",
    "    total_samples = 0\n",
    "\n",
    "    for images in dataloader:\n",
    "        # enable gradient on inputs\n",
    "        images = images.to(device).requires_grad_()\n",
    "        embeddings = gcam.forward(images)  # [batch_size, latent_dim]\n",
    "        embeddings = embeddings.view(\n",
    "            embeddings.size(0), -1)  # flatten if needed\n",
    "\n",
    "        batch_size = embeddings.size(0)\n",
    "        batch_sum = embeddings.sum(dim=0)  # sum embeddings in batch\n",
    "\n",
    "        # Update running sums (detach to avoid graph growth)\n",
    "        group_sum += batch_sum.detach()\n",
    "        group_count += batch_size\n",
    "\n",
    "        # Compute mean embedding so far\n",
    "        mean_embed = group_sum / group_count\n",
    "\n",
    "        # Define test statistic for *this batch* contribution\n",
    "        # This is a placeholder — actual stat will be computed later after both groups processed\n",
    "        # So here, just compute squared norm of mean_embed (as dummy scalar to get grads)\n",
    "        # We will later combine gradients across groups.\n",
    "        batch_test_stat = (mean_embed ** 2).sum()\n",
    "\n",
    "        # Compute gradients of batch_test_stat w.r.t input images\n",
    "        grads = torch.autograd.grad(\n",
    "            batch_test_stat, images, retain_graph=False)[0]\n",
    "\n",
    "        if grads_sum is None:\n",
    "            grads_sum = grads.detach()\n",
    "        else:\n",
    "            grads_sum += grads.detach()\n",
    "\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # Free up memory\n",
    "        del embeddings, batch_sum, mean_embed, batch_test_stat, grads, images\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Average gradients over all samples processed\n",
    "    grads_sum /= total_samples\n",
    "\n",
    "    return group_sum, group_count, grads_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_gradcam_test_stat(gcam, dataloader1, dataloader2, device):\n",
    "    # Compute sums and gradients for both groups\n",
    "    sum1, count1, grads1 = compute_group_embeddings_and_grads(\n",
    "        gcam, dataloader1, device)\n",
    "    sum2, count2, grads2 = compute_group_embeddings_and_grads(\n",
    "        gcam, dataloader2, device)\n",
    "\n",
    "    # Compute final group means\n",
    "    mean1 = sum1 / count1\n",
    "    mean2 = sum2 / count2\n",
    "\n",
    "    # Compute test statistic: squared L2 norm of difference of means\n",
    "    test_stat = ((mean1 - mean2) ** 2).sum()\n",
    "\n",
    "    # Now compute the *true* gradient of test_stat w.r.t group means\n",
    "    # gradient of test_stat w.r.t mean1 and mean2\n",
    "    grad_test_stat = 2 * (mean1 - mean2)\n",
    "\n",
    "    # Combine batch-wise accumulated gradients from groups scaled by grad_test_stat\n",
    "    # This rescales the earlier dummy grads to represent true gradient of T w.r.t inputs\n",
    "    combined_grads = grads1 * \\\n",
    "        grad_test_stat.unsqueeze(0) - grads2 * grad_test_stat.unsqueeze(0)\n",
    "\n",
    "    # combined_grads is the gradient of test_stat w.r.t input images across groups\n",
    "    # You can now use combined_grads for GradCAM visualization or further processing\n",
    "\n",
    "    return test_stat.item(), combined_grads\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# device = torch.device(\"cuda\")\n",
    "# test_stat_value, grads = main_gradcam_test_stat(gcam, dataloader_group1, dataloader_group2, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going for experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
