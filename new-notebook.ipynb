{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data and Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\n",
      "Python executable: /sc/home/arman.beykmohammadi/miniconda3/envs/P2/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The path that data has been saved is as follows:\n",
    "# /sc/dhc-cold/groups/fglippert => but the permission to dhc-cold has been denied\n",
    "IMG_DIR = \"/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256\"\n",
    "IMG_DIR = os.path.join(IMG_DIR, \"group_by_hippocampus\")\n",
    "ANNOTATION_FILE = \"adni_T1_3T_linear_annotation.csv\"\n",
    "ANNOTATION_PATH = os.path.join(IMG_DIR, ANNOTATION_FILE)\n",
    "COLS = [\"archive_fname\", \"group\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdniMRIDataset2D(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(\n",
    "            self.img_dir, self.img_labels[\"archive_fname\"].iloc[idx])\n",
    "        # print(img_path)\n",
    "        image = self.read_image(img_path)\n",
    "        label = self.img_labels[\"group\"].iloc[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = nib.load(path).get_fdata().astype(np.uint8)\n",
    "        if img.ndim == 2:\n",
    "            img = img[:, :, np.newaxis]  # HW -> HWC\n",
    "        img = img.transpose(2, 0, 1)  # HWC -> CHW\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256/group_by_hippocampus/adni_T1_3T_linear_annotation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m adni_dataset = \u001b[43mAdniMRIDataset2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mANNOTATION_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mAdniMRIDataset2D.__init__\u001b[39m\u001b[34m(self, annotations_file, img_dir, transform, target_transform)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotations_file, img_dir, transform=\u001b[38;5;28;01mNone\u001b[39;00m, target_transform=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mself\u001b[39m.img_labels = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.img_dir = img_dir\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.transform = transform\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/P2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/P2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/P2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/P2/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/P2/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256/group_by_hippocampus/adni_T1_3T_linear_annotation.csv'"
     ]
    }
   ],
   "source": [
    "adni_dataset = AdniMRIDataset2D(\n",
    "    annotations_file=ANNOTATION_PATH, img_dir=IMG_DIR)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path not available!\n",
    "As it's apparent from the previous cell, the location \"/sc/dhc-cold/groups/fglippert/adni_t1_mprage/T1_3T_coronal_slice/T1_3T_coronal_mni_linear_hippo_resolution256/group_by_hippocampus/adni_T1_3T_linear_annotation.csv\" doesn't exist!\n",
    "Then we should find the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(dataset):\n",
    "    group0 = []\n",
    "    group1 = []\n",
    "\n",
    "    for img, label in dataset:\n",
    "        if label == 0:\n",
    "            group0.append(img)\n",
    "        else:\n",
    "            group1.append(img)\n",
    "\n",
    "    group0 = np.concatenate(group0, axis=0)\n",
    "    group1 = np.concatenate(group1, axis=0)\n",
    "\n",
    "    return (group0, group1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0, group1 = get_groups(adni_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trianed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the pretariend model along with weighst\n",
    "\n",
    "- Extracting embeddings from the last layer on groups \n",
    "\n",
    "- Checking statitical test to see if they are statistically different\n",
    "\n",
    "- Uisng Grad-Cam for back-probabgating test-statistic and visualise differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "HID_DIM = 2048\n",
    "OUT_DIM = 128\n",
    "\n",
    "\n",
    "# Renet backbone\n",
    "class resnet50_fext(nn.Module):\n",
    "    def __init__(self, pretarin=True):\n",
    "        super(resnet50, self).__init__()\n",
    "        if pretarin:\n",
    "            backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0], -1)\n",
    "        return (embedding)\n",
    "\n",
    "\n",
    "# Linear model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, mlp_hid_size, proj_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.head = nn.Sequential(nn.Linear(in_dim, mlp_hid_size),\n",
    "                                  nn.BatchNorm1d(mlp_hid_size),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(mlp_hid_size, proj_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        return (x)\n",
    "\n",
    "# Byol model\n",
    "\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self, net, backbone, hid_dim, out_dim):\n",
    "        super(BYOL, self).__init__()\n",
    "        self.net = net\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.projection = MLP(in_dim=backbone.fc.in_features,\n",
    "                              mlp_hid_size=hid_dim, proj_size=out_dim)\n",
    "        self.prediction = MLP(\n",
    "            in_dim=out_dim, mlp_hid_size=hid_dim, proj_size=out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0], -1)\n",
    "        project = self.projection(embedding)\n",
    "\n",
    "        if self.net == 'target':\n",
    "            return (project)\n",
    "        predict = self.prediction(project)\n",
    "        return (predict)\n",
    "\n",
    "# SimCLR model\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, backbone, hid_dim, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        # we get representations from avg_pooling layer\n",
    "        self.encoder = torch.nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.projection = MLP(in_dim=backbone.fc.in_features,\n",
    "                              mlp_hid_size=hid_dim, proj_size=out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.encoder(x)\n",
    "        embedding = embedding.view(embedding.size()[0], -1)\n",
    "        project = self.projection(embedding)\n",
    "        return (project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/dhc/home/masoumeh.javanbakhat/netstore-old/Baysian/3D/Explainability/Retina_Codes\"\n",
    "\n",
    "root_dir = Path(base_path)\n",
    "\n",
    "checkpoint_dir = root_dir / 'self_supervised' / 'simclr' / 'simclr_ckpts'\n",
    "\n",
    "pre_exp = 2\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_dir_last = os.path.join(checkpoint_dir, f'{pre_exp}_last_sclr.pt')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dict = torch.load(sam_dir_last, map_location=device)\n",
    "\n",
    "# Load model\n",
    "\n",
    "backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "model = SimCLR(backbone, hid_dim=2048, out_dim=128).to(device)\n",
    "\n",
    "model.load_state_dict(state_dict['model'])\n",
    "\n",
    "encoder = torch.nn.Sequential(*list(model.children())[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I can add a FC layer if I want to reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I want to use a smaller dimension I can use a FC layer as follows\n",
    "\n",
    "\n",
    "reduced_dim = 512  # You can adjust this as needed, e.g., 512, 256, 128\n",
    "fc_layer = nn.Linear(2048, reduced_dim)\n",
    "\n",
    "# it outputs hidden representations of dimension 512\n",
    "reduced_encoder = nn.Sequential(encoder, fc_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "encoder_output = encoder(sample_input)\n",
    "# This removes the extra dimension, if any\n",
    "encoder_output = encoder_output.squeeze()\n",
    "latent_representation = fc_layer(encoder_output)\n",
    "print(latent_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To get embeddings I can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_numpy(encoder, images_np, device, gr=1, batch_size=32):\n",
    "    \"\"\"Takes a numpy array of images, extract embedding vectors, return embeddings\"\"\"\n",
    "    encoder.eval()\n",
    "    embeddings_list = []\n",
    "\n",
    "    # ImageNet mean and std values\n",
    "    IMAGENET_MEAN = torch.tensor(\n",
    "        [0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    IMAGENET_STD = torch.tensor(\n",
    "        [0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    # Convert numpy array to torch tensor and move to device\n",
    "    # Assuming images_np is of shape (n_samples, 1, 256, 256)\n",
    "    images_tensor = torch.tensor(images_np).unsqueeze(1).to(device).float()\n",
    "    images_tensor = images_tensor.repeat(\n",
    "        1, 3, 1, 1)  # create image with 3 channels\n",
    "\n",
    "    # Apply ImageNet normalization\n",
    "    images_tensor = (images_tensor - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "    reduced_dim = 512  # You can adjust this as needed, e.g., 512, 256, 128\n",
    "    fc_layer = nn.Linear(2048, reduced_dim)\n",
    "\n",
    "    print(f'shape of tensor:{images_tensor.shape}')\n",
    "    # If you want to process the data in batches\n",
    "    num_samples = images_tensor.size(0)\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = images_tensor[i:i + batch_size]\n",
    "\n",
    "        # Ensure the batch is on the right device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Get embeddings from the model\n",
    "        with torch.no_grad():\n",
    "            embeddings = encoder(batch)\n",
    "            # Flatten embeddings if needed\n",
    "            embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "            # embeddings = fc_layer(embeddings)\n",
    "        # Append the embeddings\n",
    "        embeddings_list.append(embeddings.cpu().numpy())\n",
    "\n",
    "    n = embeddings_list[0].shape[1]\n",
    "\n",
    "    path_embed = os.path.join('./adni_embed', f'embed_gr{gr}_{n}_nr.npy')\n",
    "\n",
    "    gr_embed = np.vstack(embeddings_list)\n",
    "\n",
    "    print(f'shape of embeddings:{gr_embed.shape}')\n",
    "\n",
    "    np.save(path_embed, gr_embed)\n",
    "\n",
    "    return gr_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group0_embed_nr = get_embeddings_from_numpy(\n",
    "    encoder=encoder, images_np=group0, device=device, gr=1, batch_size=64)\n",
    "\n",
    "group1_embed_nr = get_embeddings_from_numpy(\n",
    "    encoder=encoder, images_np=group1, device=device, gr=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now I can define path and save embeddings as follows:\n",
    "\n",
    "- Note that you should change path as I used this path befoe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gr1_embed = os.path.join(path, 'embed_gr1.npy')\n",
    "\n",
    "path_gr2_embed = os.path.join(path, 'embed_gr2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's save embeddings of each group\n",
    "\n",
    "np.save(path_gr1_embed, group0_embed)\n",
    "\n",
    "np.save(path_gr2_embed, group1_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDTest:\n",
    "    def __init__(self, features_X, features_Y, n_perm=1000):\n",
    "\n",
    "        self.n_perm = n_perm\n",
    "        self.features_X = features_X\n",
    "        self.features_Y = features_Y\n",
    "\n",
    "    def _compute_mmd(self, features_X, features_Y):\n",
    "\n",
    "        mean_fX = features_X.mean(0)\n",
    "        mean_fY = features_Y.mean(0)\n",
    "        D = mean_fX - mean_fY\n",
    "        statistic = np.linalg.norm(D)**2\n",
    "        return statistic\n",
    "\n",
    "    def _compute_p_value(self):\n",
    "\n",
    "        # compute real test statistic\n",
    "        stat = self._compute_mmd(self.features_X, self.features_Y)\n",
    "        n, m = len(self.features_X), len(self.features_Y)\n",
    "        l = n + m\n",
    "        features_Z = np.vstack((self.features_X, self.features_Y))\n",
    "\n",
    "        # compute null samples\n",
    "        resampled_vals = np.empty(self.n_perm)\n",
    "        for i in range(self.n_perm):\n",
    "            index = np.random.permutation(l)  # it permutes indices from 0 to l\n",
    "            feats_X, feats_Y = features_Z[index[:n]], features_Z[index[n:]]\n",
    "            resampled_vals[i] = self._compute_mmd(feats_X, feats_Y)\n",
    "\n",
    "        resampled_vals.sort()\n",
    "        # p_val = np.mean(stat < resampled_vals)\n",
    "        p_val = (np.sum(stat <= resampled_vals)+1)/(self.n_perm+1)\n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd = MMDTest(group0_embed, group1_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1_embed = np.load(path_gr1_embed)\n",
    "\n",
    "gr2_embed = np.load(path_gr2_embed)\n",
    "\n",
    "print(gr1_embed.shape)\n",
    "\n",
    "print(gr2_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting mean embeddings and backprobagating test-statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradcam import GradCAM\n",
    "import gradcam\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my first code (old code)\n",
    "\n",
    "def get_mean_embeddings(gcam, dataloader, device, latent_dim=2048):\n",
    "    \"\"\"Takes dataloader of each group, extract embedding vectors, return mean embeddings\"\"\"\n",
    "    # Initialize accumulators for healthy and unhealthy groups\n",
    "    sum_f = torch.zeros_like(torch.zeros(latent_dim)).to(device)\n",
    "    count_f = 0  # Count of samples in each group\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = gcam.forward(images)\n",
    "        embeddings = embeddings.view(embeddings.size()[0], -1)\n",
    "        sum_f += embeddings.sum(dim=0)  # Sum of embeddings for this batch\n",
    "        count_f += embeddings.size(0)\n",
    "    mean_embed = sum_f / count_f if count_f > 0 else torch.zeros_like(sum_f)\n",
    "    del sum_f, embeddings, images\n",
    "    torch.cuda.empty_cache()\n",
    "    return mean_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(group, device):\n",
    "    # Assuming images_np is of shape (n_samples, 1, 256, 256)\n",
    "    group_tensor = torch.tensor(group).unsqueeze(1).to(device).float()\n",
    "    group_tensor = group_tensor.repeat(\n",
    "        1, 3, 1, 1)  # create image with 3 channels\n",
    "\n",
    "    IMAGENET_MEAN = torch.tensor(\n",
    "        [0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    IMAGENET_STD = torch.tensor(\n",
    "        [0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    # Apply ImageNet normalization\n",
    "    group_tensor = (group_tensor - IMAGENET_MEAN) / IMAGENET_STD\n",
    "\n",
    "    return (group_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(group0, group1, device, bs=64):\n",
    "    group0_tensor = convert_to_tensor(group0, device)\n",
    "    group1_tensor = convert_to_tensor(group1, device)\n",
    "\n",
    "    print(group0_tensor.shape)\n",
    "    print(group1_tensor.shape)\n",
    "\n",
    "    group0_loader = DataLoader(\n",
    "        group0_tensor, batch_size=bs, shuffle=False, drop_last=True)\n",
    "    group1_loader = DataLoader(\n",
    "        group1_tensor, batch_size=bs, shuffle=False, drop_last=True)\n",
    "\n",
    "    return (group0_loader, group1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr0_nr_loader, gr1_nr_loader = get_loader(gr0_100, gr1_100, device, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprobagating test statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprobagate_statistics(model, group0, group1, target_layer, bs, device):\n",
    "    \"\"\"Calculate the test statistic for different groups of DR.\"\"\"\n",
    "    # convert numpy groups to tensors\n",
    "    group0_loader, group1_loader = get_loader(group0, group1, device, bs=64)\n",
    "\n",
    "    gcam = GradCAM(model, target_layer=target_layer, relu=True, device=device)\n",
    "    # Calculate mean embeddings\n",
    "    group0_mean = get_mean_embeddings(gcam, group0_loader, device)\n",
    "    group1_mean = get_mean_embeddings(gcam, group1_loader, device)\n",
    "    D = group0_mean - group1_mean\n",
    "    print(f'group0_mean:{group0_mean.shape}')\n",
    "    print(f'group1_mean:{group1_mean.shape}')\n",
    "    test_statistic = torch.norm(D, p=2)\n",
    "    return (test_statistic, D, gcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attributions(dataloader, gcam, device, backprop_value):\n",
    "    \"\"\"Process and return GradCAM attributions in batches.\"\"\"\n",
    "    attributions_list = []\n",
    "    embed_list = []  # save embeddings as numpy array\n",
    "    # Compute attribution maps for each group\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = gcam.forward(images)\n",
    "        embeddings = embeddings.view(\n",
    "            embeddings.size()[0], -1).cpu().data.numpy()\n",
    "        embed_list.append(embeddings)\n",
    "        del embeddings\n",
    "        gcam.backward(backprop_value)\n",
    "        attributions = gcam.generate()\n",
    "        attributions = attributions.squeeze().cpu().data.numpy()\n",
    "        attributions_list.append(attributions)\n",
    "    return np.vstack(attributions_list), np.vstack(embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, group0, group1, device, target_layer, bs=64, backprop_type='test_statistic', latent_dim_idx=None):\n",
    "    \"\"\"Main experiment function.\"\"\"\n",
    "    test_statistic, D, gcam = backprobagate_statistics(\n",
    "        model, group0, group1, target_layer, bs, device)\n",
    "\n",
    "    backprop_value = test_statistic\n",
    "\n",
    "    group0_loader, group1_loader = get_loader(group0, group1, device)\n",
    "\n",
    "    group0_attr, group0_embed = process_attributions(\n",
    "        group0_loader, gcam, backprop_value)\n",
    "    group1_attr, group1_embed = process_attributions(\n",
    "        group1_loader, gcam, backprop_value)\n",
    "\n",
    "    # save_attributions(group0_attr, group1_attr,latent_dim_idx)\n",
    "\n",
    "    print(f'gr1:{group0_attr.shape}')\n",
    "    print(f'gr2:{group2_attr.shape}')\n",
    "\n",
    "    return (group0_attr, group1_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imorting heatmaps and looking at them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroGradientError(Exception):\n",
    "    \"\"\"Custom exception to handle cases where the gradient is zero.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def save_cam_with_alpha(image, gcam, alpha=0.5):\n",
    "\n",
    "    # Convert grayscale image to 3 channels if needed\n",
    "    if len(image.shape) == 2:  # Grayscale image (H, W)\n",
    "        image = np.stack([image] * 3, axis=-1)  # Convert to (H, W, 3)\n",
    "\n",
    "    # Normalize the Grad-CAM values to [0, 1]\n",
    "    # Normalize the Grad-CAM values to [0, 1], handling zero gradients\n",
    "    gcam_min = np.min(gcam)\n",
    "    gcam_max = np.max(gcam)\n",
    "\n",
    "    try:\n",
    "        if gcam_max == gcam_min:  # If all values are zero, raise an error\n",
    "            raise ZeroGradientError(\n",
    "                \"Gradient map contains only zero values, cannot overlay.\")\n",
    "        # Normalize gradient map\n",
    "        gcam = (gcam - gcam_min) / (gcam_max - gcam_min)\n",
    "    except ZeroGradientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # Handle the error (for example, return the original image or skip processing)\n",
    "        return image, image  # Return the original image if error occurs\n",
    "\n",
    "    # Resize Grad-CAM to match the image dimensions (224x224)\n",
    "    # Get height and width (height, width) from image shape\n",
    "    h, w = image.shape[:2]\n",
    "    gcam_resized = np.array(Image.fromarray(\n",
    "        gcam).resize((w, h), Image.BILINEAR))\n",
    "\n",
    "    # Apply a colormap (similar to cv2.applyColorMap)\n",
    "    # Apply colormap and select RGB channels\n",
    "    gcam_colored = plt.cm.jet(gcam_resized)[:, :, :3] * 255\n",
    "    gcam_colored = gcam_colored.astype(np.uint8)\n",
    "\n",
    "    # Add Grad-CAM on top of the original image using alpha blending\n",
    "    heatmap = gcam_colored.astype(np.float64)\n",
    "\n",
    "    # checking dimension of image and heatmaps\n",
    "    print(f'heatmap:{heatmap.shape}')\n",
    "    print(f'image:{image.shape}')\n",
    "\n",
    "    overlaid_image = (alpha * heatmap + (1 - alpha) *\n",
    "                      image.astype(np.float64)).astype(np.uint8)\n",
    "\n",
    "    return image, overlaid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the 'src' directory\n",
    "src_path = os.path.abspath(\"src\")\n",
    "\n",
    "# Add 'src' to system path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New way to solve OOM problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://chatgpt.com/c/683eac2a-0d4c-8005-a5b7-71fd71ce4a9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def compute_group_embeddings_and_grads(gcam, dataloader, device):\n",
    "    \"\"\"\n",
    "    For a group, compute:\n",
    "    - accumulated sum of embeddings (detached)\n",
    "    - total number of samples\n",
    "    - accumulated gradients w.r.t. input images (detached)\n",
    "    \"\"\"\n",
    "    latent_dim = 2048  # or whatever your embedding size is\n",
    "    group_sum = torch.zeros(latent_dim, device=device)\n",
    "    group_count = 0\n",
    "    grads_sum = None\n",
    "    total_samples = 0\n",
    "\n",
    "    for images in dataloader:\n",
    "        # enable gradient on inputs\n",
    "        images = images.to(device).requires_grad_()\n",
    "        embeddings = gcam.forward(images)  # [batch_size, latent_dim]\n",
    "        embeddings = embeddings.view(\n",
    "            embeddings.size(0), -1)  # flatten if needed\n",
    "\n",
    "        batch_size = embeddings.size(0)\n",
    "        batch_sum = embeddings.sum(dim=0)  # sum embeddings in batch\n",
    "\n",
    "        # Update running sums (detach to avoid graph growth)\n",
    "        group_sum += batch_sum.detach()\n",
    "        group_count += batch_size\n",
    "\n",
    "        # Compute mean embedding so far\n",
    "        mean_embed = group_sum / group_count\n",
    "\n",
    "        # Define test statistic for *this batch* contribution\n",
    "        # This is a placeholder — actual stat will be computed later after both groups processed\n",
    "        # So here, just compute squared norm of mean_embed (as dummy scalar to get grads)\n",
    "        # We will later combine gradients across groups.\n",
    "        batch_test_stat = (mean_embed ** 2).sum()\n",
    "\n",
    "        # Compute gradients of batch_test_stat w.r.t input images\n",
    "        grads = torch.autograd.grad(\n",
    "            batch_test_stat, images, retain_graph=False)[0]\n",
    "\n",
    "        if grads_sum is None:\n",
    "            grads_sum = grads.detach()\n",
    "        else:\n",
    "            grads_sum += grads.detach()\n",
    "\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # Free up memory\n",
    "        del embeddings, batch_sum, mean_embed, batch_test_stat, grads, images\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Average gradients over all samples processed\n",
    "    grads_sum /= total_samples\n",
    "\n",
    "    return group_sum, group_count, grads_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_gradcam_test_stat(gcam, dataloader1, dataloader2, device):\n",
    "    # Compute sums and gradients for both groups\n",
    "    sum1, count1, grads1 = compute_group_embeddings_and_grads(\n",
    "        gcam, dataloader1, device)\n",
    "    sum2, count2, grads2 = compute_group_embeddings_and_grads(\n",
    "        gcam, dataloader2, device)\n",
    "\n",
    "    # Compute final group means\n",
    "    mean1 = sum1 / count1\n",
    "    mean2 = sum2 / count2\n",
    "\n",
    "    # Compute test statistic: squared L2 norm of difference of means\n",
    "    test_stat = ((mean1 - mean2) ** 2).sum()\n",
    "\n",
    "    # Now compute the *true* gradient of test_stat w.r.t group means\n",
    "    # gradient of test_stat w.r.t mean1 and mean2\n",
    "    grad_test_stat = 2 * (mean1 - mean2)\n",
    "\n",
    "    # Combine batch-wise accumulated gradients from groups scaled by grad_test_stat\n",
    "    # This rescales the earlier dummy grads to represent true gradient of T w.r.t inputs\n",
    "    combined_grads = grads1 * \\\n",
    "        grad_test_stat.unsqueeze(0) - grads2 * grad_test_stat.unsqueeze(0)\n",
    "\n",
    "    # combined_grads is the gradient of test_stat w.r.t input images across groups\n",
    "    # You can now use combined_grads for GradCAM visualization or further processing\n",
    "\n",
    "    return test_stat.item(), combined_grads\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# device = torch.device(\"cuda\")\n",
    "# test_stat_value, grads = main_gradcam_test_stat(gcam, dataloader_group1, dataloader_group2, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going for experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
